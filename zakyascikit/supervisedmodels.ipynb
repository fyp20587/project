{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf0cd6d",
   "metadata": {},
   "source": [
    "# Supervised Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "244adf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zakya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/zakya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    }
   ],
   "source": [
    "#[Source 1] Scikit-learn: Machine Learning in Python, Pedregosa et al., 2011. [Accessed 23/01/2024]\n",
    "#[Source 2] Palivela, A.K. (2021). Sentiment Analysis with Machine Learning. [Online] Kaggle. Available at: [https://www.kaggle.com/code/ashokkumarpalivela/sentiment-analysis-with-machine-learning][Accessed 23/03/2024]\n",
    "#[Source 3] Appiah, E.K., 2024. Sentiment analysis using SVM, Naive Bayes & RF. [online] Kaggle. Available at: https://www.kaggle.com/code/emmanuelkwasiappiah/sentiment-analysis-using-svm-naive-bayes-rf/notebook [Accessed 10/04/2024]\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "dataf = pd.read_csv(\"processed_data.csv\")\n",
    "dataf['sentiment'] = dataf['sentiment'].str.lower()\n",
    "positive_samples = dataf[dataf['sentiment'] == 'positive']\n",
    "negative_samples = dataf[dataf['sentiment'] == 'negative']\n",
    "neutral_samples = dataf[dataf['sentiment'] == 'neutral'].sample(1900)  #undersampling for balance\n",
    "dataf = pd.concat([positive_samples, negative_samples, neutral_samples], ignore_index=True)\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))  # Remove punctuations\n",
    "    text = ''.join([i for i in text if not i.isdigit()])  # Remove digits\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    stop_words = set(stopwords.words('english'))  # Get English stop words\n",
    "    filtered_text = [word for word in tokens if word.lower() not in stop_words]  # Remove stop words\n",
    "    cleaned_text = ' '.join(filtered_text).strip()  # Join words to form the cleaned text\n",
    "    return cleaned_text\n",
    "\n",
    "#apply text cleaning\n",
    "dataf['cleaned_text'] = dataf['cleaned_text'].apply(text_cleaning)\n",
    "\n",
    "#splitting the dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    dataf['cleaned_text'], dataf['sentiment'], test_size=0.2, random_state=42\n",
    ")\n",
    "y_train = train_labels\n",
    "\n",
    "def preprocess_and_feature_select(train_texts, test_texts, train_labels):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.25, ngram_range=(1, 2))\n",
    "    train_vectors = vectorizer.fit_transform(train_texts)\n",
    "    test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "    selector = SelectKBest(chi2, k='all')\n",
    "    train_features = selector.fit_transform(train_vectors, train_labels)\n",
    "    test_features = selector.transform(test_vectors)\n",
    "    \n",
    "    return train_features, test_features, vectorizer, selector\n",
    "\n",
    "#unpack the returned values\n",
    "X_train_selected, X_test_selected, vect, fs_selc = preprocess_and_feature_select(train_texts, test_texts, train_labels)\n",
    "\n",
    "\n",
    "def model(features_train, labels_train, classifier_type, config):\n",
    "    grid_search = GridSearchCV(config[classifier_type], config['params'][classifier_type], n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "config = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'SVC': SVC(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'params': {\n",
    "        'MultinomialNB': {'alpha': (0.1, 0.01, 0.001)},\n",
    "        'SVC': {'C': (1, 10, 100), 'kernel': ('linear', 'poly')},\n",
    "        'RandomForest': {'n_estimators': (50, 150, 250), 'max_depth': (None, 15, 25)}\n",
    "    }\n",
    "}\n",
    "\n",
    "best_model_nb, best_params_nb = model(X_train_selected, y_train, 'MultinomialNB', config)\n",
    "best_model_svc, best_params_svc = model(X_train_selected, y_train, 'SVC', config)\n",
    "best_model_rf, best_params_rf = model(X_train_selected, y_train, 'RandomForest', config)\n",
    "\n",
    "#function is for evaluating classifier\n",
    "def evaluate_classifier(clf, test_texts, test_labels):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    predictions = clf.predict(test_texts)\n",
    "    metrics = {\n",
    "        'Accuracy': float(accuracy_score(test_labels, predictions)),\n",
    "        'Precision': float(precision_score(test_labels, predictions, average='weighted')),\n",
    "        'Recall': float(recall_score(test_labels, predictions, average='weighted')),\n",
    "        'F1 Score': float(f1_score(test_labels, predictions, average='weighted'))\n",
    "    }\n",
    "    return metrics\n",
    "#evaluation\n",
    "accuracy_nb, precision_nb, recall_nb, f1_nb = evaluate_classifier(best_model_nb, X_test_selected, test_labels)\n",
    "accuracy_svc, precision_svc, recall_svc, f1_svc = evaluate_classifier(best_model_svc, X_test_selected, test_labels)\n",
    "accuracy_rf, precision_rf, recall_rf, f1_rf = evaluate_classifier(best_model_rf, X_test_selected, test_labels)\n",
    "\n",
    "def predict_with_model(input_text, model):\n",
    "    transformed_text = fs_selc.transform(vect.transform([input_text]))\n",
    "    prediction = model.predict(transformed_text)\n",
    "    return prediction\n",
    "\n",
    "#trying out the system\n",
    "example_text = \"Obama is really good. He is for the people and made good decisions\"\n",
    "output_of_prediction = predict_with_model(example_text, best_model_nb)\n",
    "print(\"Prediction using best_model_nb:\", output_of_prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
